{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "\n",
    "The condition or quality of being true, correct, or exact; freedom from error or defect is accuracy.\n",
    "\n",
    "Accuracy, the proportion of correct classifications among all classifications, is very simple and very \"intuitive\" measure.\n",
    "\n",
    "Model accuracy refers to the percentage of predictions generated by a particular model that are accurate or correct.\n",
    "\n",
    "    Accuracy = No of correct predictions / Total no of predictions\n",
    "\n",
    "**Why Accuracy is not a good metric to evaluate classifiers ?**\n",
    "\n",
    "* Accuracy is not always the best to evaluate classifier (especially in skewed datasets).\n",
    "* Evaluating a classifier is trckier than evaluating a regressor.\n",
    "* Accuracy can be misleading or insufficient in some scenarios, such as when the data set is imbalanced or has more than two classes.\n",
    "* It does not take into account the distribution of the classes in the data. If one class is much more frequent than the other, accuracy can be high even if the model is not very good at predicting the rare class.\n",
    "\n",
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a two-dimensional matrix used in classification experiments to evaluate the performance of a system by showing the number of correctly and wrongly classified data, helping to identify which classes of data are most often mispaceed.\n",
    "\n",
    "                        Predicted\n",
    "    Actual\n",
    "                       Pos      Neg                     \n",
    "                     _________________\n",
    "    Positive        |        |        |\n",
    "                    |   TP   |   FP   |\n",
    "                     _________________\n",
    "    Negative        |        |        |\n",
    "                    |   FN   |   TN   |\n",
    "                     _________________\n",
    "\n",
    "\n",
    "* Much better way to evaluate classifiers\n",
    "* Count the no of times instances of class A are classified (or confused) as class B\n",
    "\n",
    "\n",
    "**Creating Confusion Matrix**\n",
    "* Get the set of predictions, so they can be compared to the actual targets.\n",
    "* Each row in as CM represents an actual class, whilw each column presents a predicted class.\n",
    "* Best classifier is the one having only true positives and true negatives, ie. CM would've nonzero values only on its main diagonal(top left to top bottom).\n",
    "\n",
    "\n",
    "_Metrics based on Confusion Matrix Data_:\n",
    "\n",
    "    Accuacy = TP+TN / TP+TN+FP+FN \n",
    "\n",
    "    Precision = TP / TP+FP \n",
    "\n",
    "    Recall = TP / TP+FN \n",
    "\n",
    "    F1 Score = 2*Precision*Recall / Precision+Recall \n",
    "\n",
    "    Specificity = TN / TN+FP \n",
    "\n",
    "    Type1 Error = FP / TN+FP \n",
    "\n",
    "    Type2 Error = FN / TP+FN \n",
    "\n",
    "------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "\n",
    "Precision is one indicator of a machine learning model's performance â€“ the quality of a positive prediction made by the model. \n",
    "\n",
    "In simple terms -- What percent of positive predictions made were correct ?\n",
    "\n",
    "Precision refers to the number of true positives divided by the total number of positive predictions (i.e., the number of true positives plus the number of false positives).\n",
    "\n",
    "    Precision = TP / TP+FP \n",
    "\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall \n",
    "\n",
    "Recall is a metric that measures how often a machine learning model correctly identifies positive instances (true positives) from all the actual positive samples in the dataset. \n",
    "\n",
    "In simple terms -- What percent of Actual Positive values were correctly classified by the classifier ?\n",
    "\n",
    "It can be calculated by dividing the number of true positives by the number of positive instances.\n",
    "\n",
    "    Recall = TP / TP+FN \n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "\n",
    "The F1 Score is a crucial metric in machine learning that provides a balanced measure of a model's precision and recall.\n",
    "This metric is particularly useful in scenarios where class distributions are imbalanced.\n",
    "\n",
    "* It's convenient to combine the performance of a classifier (precision & recall) into a single metric called F1 Score.\n",
    "* It's the harmonic mean of Precision and Recall.\n",
    "\n",
    "F1-score is the evaluation matrix that combines two matrices: Precision and Recall, into a single metric by taking their harmonic mean. \n",
    "\n",
    "In simple terms, the f1 score is the weighted average mean of Precision and Recall.\n",
    "\n",
    "    F1 Score = 1 / ( 1/Precision + 1/Recall ) / 2\n",
    "    F1 Score = 2 / ( 1/Precision + 1/Recall )\n",
    "    F1 Score = 2 / (Precision+Recall / Precision*Recall)\n",
    "\n",
    "    F1 Score = 2*Precision*Recall / Precision+Recall \n",
    "\n",
    "\n",
    "**Why Harmonic Mean ?** Because the harmonic mean ensures that if either precision or recall is very low, the F1 score will also be low, preventing misleadingly high scores and ensuring a more accurate reflection of the model's performance.\n",
    "\n",
    "\n",
    "    High F1 Score --> High Precision  &  High Recall\n",
    "\n",
    "    Low F1 Score --> High/Low Precision  &  Low/High Recall\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
